{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORK: Basic regression and classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>[Pre-Work]</h3><br>\n",
    "<p>Scikit-learnのデータセットにはどのようなものがあるか確認し、指定するデータセットに対してモデル構築とアルゴリズムを検証してください。</p>\n",
    "<p>5.2. Toy datasets</p>\n",
    "•http://scikit-learn.org/stable/datasets/index.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>[Work-1]</h3><br>\n",
    "<p>ボストンの住宅価格についての回帰のデータセットを検証します。</p>\n",
    "- 線形回帰（ＯＬＳ）およびリッジ回帰を用いてモデルのトレーニングをしましょう\n",
    "- 過学習を軽減するためリッジ回帰については任意のαの値を2通り試しましょう(ex. alpha=1.0, alpha=10.0)\n",
    "- ホールドアウト法を用いてトレインデータ8割、テストデータ2割を用意しましょう\n",
    "- R2スコアを各アルゴリズムで比較し最も性能のよりアルゴリズムを検討しましょう\n",
    "- R2スコアの最も高いアルゴリズムを用いてテストデータの1番目の住宅価格を求めましょう<br>\n",
    "※実際の1番目のテストデータの住宅価格からどの程度離れているでしょうか"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットを確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------\n",
      "X shape: (506,13)\n",
      "y shape: (506,1)\n",
      "----------------------------------------------------------------------------------------\n",
      "             MEDV\n",
      "count  506.000000\n",
      "mean    22.532806\n",
      "std      9.197104\n",
      "min      5.000000\n",
      "25%     17.025000\n",
      "50%     21.200000\n",
      "75%     25.000000\n",
      "max     50.000000\n",
      "----------------------------------------------------------------------------------------\n",
      "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
      "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
      "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
      "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
      "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
      "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
      "\n",
      "   PTRATIO       B  LSTAT  MEDV  \n",
      "0     15.3  396.90   4.98  24.0  \n",
      "1     17.8  396.90   9.14  21.6  \n",
      "2     17.8  392.83   4.03  34.7  \n",
      "3     18.7  394.63   2.94  33.4  \n",
      "4     18.7  396.90   5.33  36.2  \n",
      "----------------------------------------------------------------------------------------\n",
      "Boston House Prices dataset\n",
      "===========================\n",
      "\n",
      "Notes\n",
      "------\n",
      "Data Set Characteristics:  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive\n",
      "    \n",
      "    :Median Value (attribute 14) is usually the target\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "http://archive.ics.uci.edu/ml/datasets/Housing\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      "**References**\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import the boston house-prices datase for regression\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "dataset = load_boston()\n",
    "\n",
    "# set dataframe\n",
    "X = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
    "y = pd.DataFrame(dataset.target, columns=['MEDV'])\n",
    "\n",
    "# check the shape\n",
    "print('----------------------------------------------------------------------------------------')\n",
    "print('X shape: (%i,%i)' %X.shape)\n",
    "print('y shape: (%i,%i)' %y.shape)\n",
    "print('----------------------------------------------------------------------------------------')\n",
    "print(y.describe())\n",
    "print('----------------------------------------------------------------------------------------')\n",
    "print(X.join(y).head())\n",
    "print('----------------------------------------------------------------------------------------')\n",
    "print(dataset.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下の[----------]にコードを記述してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS  : 0.763481\n",
      "Ridge: 0.763468\n",
      "Ridge: 0.761724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hiroa\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "C:\\Users\\hiroa\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "C:\\Users\\hiroa\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:34: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "C:\\Users\\hiroa\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:37: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression,Ridge # 線形回帰およびリッジ回帰のライブラリ\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# set dataframe\n",
    "dataset = load_boston()\n",
    "X = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
    "y = pd.DataFrame(dataset.target, columns=['y'])\n",
    "#y = pd.DataFrame(dataset.target,name='y') # series型のままの方がよい\n",
    "\n",
    "# cross-validation(holdout)\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.20, random_state=1)\n",
    "\n",
    "# make pipelines for modeling # これが100個、200個になってもかけるようにする方がいいですよ。\n",
    "# OLS\n",
    "pipe_ols =Pipeline([('scl',StandardScaler()),\n",
    "                    ('est',LinearRegression())])\n",
    "# Ridge\n",
    "pipe_ridge_1 =Pipeline([('scl',StandardScaler()),\n",
    "                       ('est',Ridge(alpha=1.0, random_state=0))])\n",
    "\n",
    "pipe_ridge_2 =Pipeline([('scl',StandardScaler()),\n",
    "                       ('est',Ridge(alpha=10.0, random_state=0))])\n",
    "\n",
    "# build models\n",
    "pipe_ols.fit(X_train, y_train.as_matrix().ravel())\n",
    "pipe_ridge_1.fit(X_train, y_train.as_matrix().ravel())\n",
    "pipe_ridge_2.fit(X_train, y_train.as_matrix().ravel())\n",
    "\n",
    "# get R2 score\n",
    "y_true = y_test.as_matrix().ravel()\n",
    "\n",
    "# print the performance\n",
    "# ここにR2スコアを表示するコードを記述してください。\n",
    "print('OLS  : %.6f' % r2_score(y_test,pipe_ols.predict(X_test)))\n",
    "print('Ridge: %.6f' % r2_score(y_test,pipe_ridge_1.predict(X_test)))\n",
    "print('Ridge: %.6f' % r2_score(y_test,pipe_ridge_2.predict(X_test)))\n",
    "\n",
    "# OLSが最も高い\n",
    "\n",
    "# Predict the first data of test data\n",
    "#test = X_test.[----------]\n",
    "#print([----------])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>[Work-2]</h3><br>\n",
    "<p>糖尿病についての回帰のデータセット</p>\n",
    "- ランダムフォレストおよび勾配ブースティングを用いてモデルのトレーニングをしましょう\n",
    "- ホールドアウト法を用いてトレインデータ8割、テストデータ2割を用意しましょう\n",
    "- R2スコアを各アルゴリズムで比較し最も性能のよりアルゴリズムを検討しましょう\n",
    "- R2スコアの最も高いアルゴリズムを用いてテストデータの1番目の糖尿病指標を求めましょう<br>\n",
    "※実際の1番目のテストデータの糖尿病指標からどの程度離れているでしょうか<br>\n",
    "- 他にもっと性能の良いアルゴリズムがないか探してみましょう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットを確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------\n",
      "X shape: (442,10)\n",
      "y shape: (442,1)\n",
      "----------------------------------------------------------------------------------------\n",
      "                y\n",
      "count  442.000000\n",
      "mean   152.133484\n",
      "std     77.093005\n",
      "min     25.000000\n",
      "25%     87.000000\n",
      "50%    140.500000\n",
      "75%    211.500000\n",
      "max    346.000000\n",
      "----------------------------------------------------------------------------------------\n",
      "        age       sex       bmi        bp        s1        s2        s3  \\\n",
      "0  0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
      "1 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
      "2  0.085299  0.050680  0.044451 -0.005671 -0.045599 -0.034194 -0.032356   \n",
      "3 -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
      "4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
      "\n",
      "         s4        s5        s6      y  \n",
      "0 -0.002592  0.019908 -0.017646  151.0  \n",
      "1 -0.039493 -0.068330 -0.092204   75.0  \n",
      "2 -0.002592  0.002864 -0.025930  141.0  \n",
      "3  0.034309  0.022692 -0.009362  206.0  \n",
      "4 -0.002592 -0.031991 -0.046641  135.0  \n",
      "----------------------------------------------------------------------------------------\n",
      "Diabetes dataset\n",
      "================\n",
      "\n",
      "Notes\n",
      "-----\n",
      "\n",
      "Ten baseline variables, age, sex, body mass index, average blood\n",
      "pressure, and six blood serum measurements were obtained for each of n =\n",
      "442 diabetes patients, as well as the response of interest, a\n",
      "quantitative measure of disease progression one year after baseline.\n",
      "\n",
      "Data Set Characteristics:\n",
      "\n",
      "  :Number of Instances: 442\n",
      "\n",
      "  :Number of Attributes: First 10 columns are numeric predictive values\n",
      "\n",
      "  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n",
      "\n",
      "  :Attributes:\n",
      "    :Age:\n",
      "    :Sex:\n",
      "    :Body mass index:\n",
      "    :Average blood pressure:\n",
      "    :S1:\n",
      "    :S2:\n",
      "    :S3:\n",
      "    :S4:\n",
      "    :S5:\n",
      "    :S6:\n",
      "\n",
      "Note: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times `n_samples` (i.e. the sum of squares of each column totals 1).\n",
      "\n",
      "Source URL:\n",
      "http://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n",
      "\n",
      "For more information see:\n",
      "Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n",
      "(http://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import the diabetes dataset for regression\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "dataset = load_diabetes()\n",
    "\n",
    "# set dataframe\n",
    "X = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
    "y = pd.DataFrame(dataset.target, columns=['y'])\n",
    "\n",
    "# check the shape\n",
    "print('----------------------------------------------------------------------------------------')\n",
    "print('X shape: (%i,%i)' %X.shape)\n",
    "print('y shape: (%i,%i)' %y.shape)\n",
    "print('----------------------------------------------------------------------------------------')\n",
    "print(y.describe())\n",
    "print('----------------------------------------------------------------------------------------')\n",
    "print(X.join(y).head())\n",
    "print('----------------------------------------------------------------------------------------')\n",
    "print(dataset.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下の[----------]を適切な命令に置き換えて下さい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS                      : 0.438436\n",
      "Ridge                    : 0.436251\n",
      "RandomForest             : 0.200165\n",
      "GradientBoostingRegressor: 0.297574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hiroa\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:36: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "C:\\Users\\hiroa\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:37: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "C:\\Users\\hiroa\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:38: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "C:\\Users\\hiroa\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:39: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "C:\\Users\\hiroa\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression,Ridge # 線形回帰およびリッジ回帰のライブラリ\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor# ランダムフォレストおよび勾配ブースティングのライブラリ\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# set dataframe\n",
    "dataset = load_diabetes()\n",
    "X = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
    "y = pd.DataFrame(dataset.target, columns=['y'])\n",
    "\n",
    "# cross-validation(holdout)\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.20, random_state=1)\n",
    "\n",
    "# make pipelines for modeling\n",
    "\n",
    "pipe_ols =Pipeline([('scl',StandardScaler()),\n",
    "                    ('est',LinearRegression())])\n",
    "\n",
    "pipe_ridge =Pipeline([('scl',StandardScaler()),\n",
    "                       ('est',Ridge(alpha=1.0, random_state=1))])\n",
    "\n",
    "# RandomForest\n",
    "pipe_rf =Pipeline([('scl',StandardScaler()),\n",
    "                    ('est',RandomForestRegressor(random_state=1))])\n",
    "# GradientBoosting\n",
    "pipe_gbr = Pipeline([('scl',StandardScaler()),\n",
    "                    ('est',GradientBoostingRegressor(random_state=1))])\n",
    "\n",
    "# build models\n",
    "pipe_ols.fit(X_train, y_train.as_matrix().ravel())\n",
    "pipe_ridge.fit(X_train, y_train.as_matrix().ravel())\n",
    "pipe_rf.fit(X_train, y_train.as_matrix().ravel())\n",
    "pipe_gbr.fit(X_train, y_train.as_matrix().ravel())\n",
    "\n",
    "# get R2 score\n",
    "y_true = y_test.as_matrix().ravel()\n",
    "\n",
    "# print the performance\n",
    "# ここにR2スコアを表示するコードを記述してください。\n",
    "print('OLS                      : %.6f' % r2_score(y_test,pipe_ols.predict(X_test)))\n",
    "print('Ridge                    : %.6f' % r2_score(y_test,pipe_ridge.predict(X_test)))\n",
    "print('RandomForest             : %.6f' % r2_score(y_test,pipe_rf.predict(X_test)))\n",
    "print('GradientBoostingRegressor: %.6f' % r2_score(y_test,pipe_gbr.predict(X_test)))\n",
    "\n",
    "## Predict the first data of test data\n",
    "#test = X_test.[----------]\n",
    "#print([----------])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>[Work-3]</h3><br>\n",
    "<p>乳がんについての分類のデータセットを検証します。</p>\n",
    "- ロジスティック回帰およびランダムフォレストを用いてモデルのトレーニングをしましょう\n",
    "- ホールドアウト法を用いてトレインデータ8割、テストデータ2割としましょう\n",
    "- 正解率（Accuracy）を各アルゴリズムで比較しましょう\n",
    "- 正解率（Accuracy）の最も高いアルゴリズムを用いてテストデータの1番目が良性か悪性か判別しましょう\n",
    "- 適合率（Precision）、再現率（Recall）、F値（F-measure）についてもアルゴリズム性能を比較してみましょう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットを確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------\n",
      "X shape: (569,30)\n",
      "y shape: (569,1)\n",
      "----------------------------------------------------------------------------------------\n",
      "y\n",
      "0    212\n",
      "1    357\n",
      "dtype: int64\n",
      "y=0 means Marignant(悪性), y=1 means Benign(良性):\n",
      "----------------------------------------------------------------------------------------\n",
      "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
      "0        17.99         10.38          122.80     1001.0          0.11840   \n",
      "1        20.57         17.77          132.90     1326.0          0.08474   \n",
      "2        19.69         21.25          130.00     1203.0          0.10960   \n",
      "3        11.42         20.38           77.58      386.1          0.14250   \n",
      "4        20.29         14.34          135.10     1297.0          0.10030   \n",
      "\n",
      "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
      "0           0.27760          0.3001              0.14710         0.2419   \n",
      "1           0.07864          0.0869              0.07017         0.1812   \n",
      "2           0.15990          0.1974              0.12790         0.2069   \n",
      "3           0.28390          0.2414              0.10520         0.2597   \n",
      "4           0.13280          0.1980              0.10430         0.1809   \n",
      "\n",
      "   mean fractal dimension ...  worst texture  worst perimeter  worst area  \\\n",
      "0                 0.07871 ...          17.33           184.60      2019.0   \n",
      "1                 0.05667 ...          23.41           158.80      1956.0   \n",
      "2                 0.05999 ...          25.53           152.50      1709.0   \n",
      "3                 0.09744 ...          26.50            98.87       567.7   \n",
      "4                 0.05883 ...          16.67           152.20      1575.0   \n",
      "\n",
      "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
      "0            0.1622             0.6656           0.7119                0.2654   \n",
      "1            0.1238             0.1866           0.2416                0.1860   \n",
      "2            0.1444             0.4245           0.4504                0.2430   \n",
      "3            0.2098             0.8663           0.6869                0.2575   \n",
      "4            0.1374             0.2050           0.4000                0.1625   \n",
      "\n",
      "   worst symmetry  worst fractal dimension  y  \n",
      "0          0.4601                  0.11890  0  \n",
      "1          0.2750                  0.08902  0  \n",
      "2          0.3613                  0.08758  0  \n",
      "3          0.6638                  0.17300  0  \n",
      "4          0.2364                  0.07678  0  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load and return the breast cancer wisconsin dataset (classification).\n",
    "# The breast cancer dataset is a classic and very easy binary classification dataset.\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "dataset = load_breast_cancer()\n",
    "\n",
    "# Set dataframe\n",
    "X = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
    "y = pd.DataFrame(dataset.target, columns=['y'])\n",
    "\n",
    "# check the shape\n",
    "print('----------------------------------------------------------------------------------------')\n",
    "print('X shape: (%i,%i)' %X.shape)\n",
    "print('y shape: (%i,%i)' %y.shape)\n",
    "print('----------------------------------------------------------------------------------------')\n",
    "print(y.groupby('y').size())\n",
    "print('y=0 means Marignant(悪性), y=1 means Benign(良性):')\n",
    "print('----------------------------------------------------------------------------------------')\n",
    "print(X.join(y).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "以下の[----------]にコードを記述してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic regression: 0.982\n",
      "Accuracy of Random forest regressor: 0.947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hiroa\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "C:\\Users\\hiroa\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "# import basice apis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression # ロジスティック回帰のライブラリ\n",
    "from sklearn.ensemble import RandomForestClassifier # ランダムフォレストのライブラリ\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# import Sample Data to learn models\n",
    "dataset = load_breast_cancer()\n",
    "X = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
    "y = pd.DataFrame(dataset.target, columns=['y'])\n",
    "\n",
    "# split data for crossvalidation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# set pipelines for different algorithms\n",
    "pipe_logistic = Pipeline([('scl',StandardScaler()),\n",
    "                    ('est',LogisticRegression(random_state=1))])\n",
    "\n",
    "pipe_rf = Pipeline([('scl',StandardScaler()),\n",
    "                    ('est',RandomForestClassifier(random_state=1))])\n",
    "\n",
    "# fit & evaluation\n",
    "pipe_logistic.fit(X_train, y_train.as_matrix().ravel())\n",
    "pipe_rf.fit(X_train, y_train.as_matrix().ravel())\n",
    "\n",
    "# print the performance\n",
    "print('Accuracy of Logistic regression: %.3f' % accuracy_score(y_test, pipe_logistic.predict(X_test)))\n",
    "print('Accuracy of Random forest regressor: %.3f' % accuracy_score(y_test, pipe_rf.predict(X_test)))\n",
    "\n",
    "# Predict the first data of test data\n",
    "#test = X_test.[----------]\n",
    "#print([----------])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>[Work-4]</h3><br>\n",
    "<p>ワインについての分類のデータセットを検証します。</p>\n",
    "- K近傍法を用いてモデルのトレーニングをしましょう<br>\n",
    "　　※K近傍法はマルチクラス分類に対応しています\n",
    "- ホールドアウト法を用いてトレインデータ8割、テストデータ2割としましょう\n",
    "- 正解率（Accuracy）をデフォルト設定のアルゴリズムで求めましょう\n",
    "- デフォルト設定のアルゴリズムを用いてテストデータの1番目がどのワインか判別しましょう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "データセットを確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------\n",
      "X shape: (178,13)\n",
      "y shape: (178,1)\n",
      "----------------------------------------------------------------------------------------\n",
      "y\n",
      "0    59\n",
      "1    71\n",
      "2    48\n",
      "dtype: int64\n",
      "y=0 means wine1, y=1 means wine2, y=2 means wine3:\n",
      "----------------------------------------------------------------------------------------\n",
      "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
      "0    14.23        1.71  2.43               15.6      127.0           2.80   \n",
      "1    13.20        1.78  2.14               11.2      100.0           2.65   \n",
      "2    13.16        2.36  2.67               18.6      101.0           2.80   \n",
      "3    14.37        1.95  2.50               16.8      113.0           3.85   \n",
      "4    13.24        2.59  2.87               21.0      118.0           2.80   \n",
      "\n",
      "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
      "0        3.06                  0.28             2.29             5.64  1.04   \n",
      "1        2.76                  0.26             1.28             4.38  1.05   \n",
      "2        3.24                  0.30             2.81             5.68  1.03   \n",
      "3        3.49                  0.24             2.18             7.80  0.86   \n",
      "4        2.69                  0.39             1.82             4.32  1.04   \n",
      "\n",
      "   od280/od315_of_diluted_wines  proline  y  \n",
      "0                          3.92   1065.0  0  \n",
      "1                          3.40   1050.0  0  \n",
      "2                          3.17   1185.0  0  \n",
      "3                          3.45   1480.0  0  \n",
      "4                          2.93    735.0  0  \n"
     ]
    }
   ],
   "source": [
    "# Load and return the wine dataset (classification).\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "dataset = load_wine()\n",
    "\n",
    "# Set dataframe\n",
    "X = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
    "y = pd.DataFrame(dataset.target, columns=['y'])\n",
    "\n",
    "# check the shape\n",
    "print('----------------------------------------------------------------------------------------')\n",
    "print('X shape: (%i,%i)' %X.shape)\n",
    "print('y shape: (%i,%i)' %y.shape)\n",
    "print('----------------------------------------------------------------------------------------')\n",
    "print(y.groupby('y').size())\n",
    "print('y=0 means wine1, y=1 means wine2, y=2 means wine3:')\n",
    "print('----------------------------------------------------------------------------------------')\n",
    "print(X.join(y).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下の[----------]を適切な命令に置き換えて下さい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of KNeighborsClassifier: 0.972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hiroa\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "# import basice apis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier # K近傍法のライブラリ\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# import Sample Data to learn models\n",
    "dataset = load_wine()\n",
    "X = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
    "y = pd.DataFrame(dataset.target, columns=['y'])\n",
    "\n",
    "# split data for crossvalidation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2 , random_state=1)\n",
    "\n",
    "# set pipelines for different algorithms\n",
    "pipe_knn =  Pipeline([('scl',StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
    "                    ('est',KNeighborsClassifier(algorithm='auto', \n",
    "                                                leaf_size=30, \n",
    "                                                metric='minkowski',\n",
    "                                                metric_params=None, \n",
    "                                                n_jobs=1, \n",
    "                                                n_neighbors=5, \n",
    "                                                p=2,\n",
    "                                                weights='uniform'))])\n",
    "# fit & evaluation\n",
    "pipe_knn.fit(X_train, y_train.as_matrix().ravel())\n",
    "\n",
    "# print the performance\n",
    "# 正解率（Accuracy）を表示するコードを記述してください。\n",
    "print('Accuracy of KNeighborsClassifier: %.3f' % accuracy_score(y_test, pipe_knn.predict(X_test)))\n",
    "\n",
    "# Predict the first data of test data\n",
    "#test = X_test.[----------]\n",
    "#print([----------])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Out[9]:\n",
    "Pipeline(memory=None,\n",
    "     steps=[('scl', StandardScaler(copy=True, with_mean=True, with_std=True)), ('est', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
    "           weights='uniform'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------\n",
      "X shape: (569,30)\n",
      "y shape: (569,1)\n",
      "----------------------------------------------------------------------------------------\n",
      "y\n",
      "0    212\n",
      "1    357\n",
      "dtype: int64\n",
      "y=0 means Marignant(悪性), y=1 means Benign(良性):\n",
      "----------------------------------------------------------------------------------------\n",
      "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
      "0        17.99         10.38          122.80     1001.0          0.11840   \n",
      "1        20.57         17.77          132.90     1326.0          0.08474   \n",
      "2        19.69         21.25          130.00     1203.0          0.10960   \n",
      "3        11.42         20.38           77.58      386.1          0.14250   \n",
      "4        20.29         14.34          135.10     1297.0          0.10030   \n",
      "\n",
      "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
      "0           0.27760          0.3001              0.14710         0.2419   \n",
      "1           0.07864          0.0869              0.07017         0.1812   \n",
      "2           0.15990          0.1974              0.12790         0.2069   \n",
      "3           0.28390          0.2414              0.10520         0.2597   \n",
      "4           0.13280          0.1980              0.10430         0.1809   \n",
      "\n",
      "   mean fractal dimension ...  worst texture  worst perimeter  worst area  \\\n",
      "0                 0.07871 ...          17.33           184.60      2019.0   \n",
      "1                 0.05667 ...          23.41           158.80      1956.0   \n",
      "2                 0.05999 ...          25.53           152.50      1709.0   \n",
      "3                 0.09744 ...          26.50            98.87       567.7   \n",
      "4                 0.05883 ...          16.67           152.20      1575.0   \n",
      "\n",
      "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
      "0            0.1622             0.6656           0.7119                0.2654   \n",
      "1            0.1238             0.1866           0.2416                0.1860   \n",
      "2            0.1444             0.4245           0.4504                0.2430   \n",
      "3            0.2098             0.8663           0.6869                0.2575   \n",
      "4            0.1374             0.2050           0.4000                0.1625   \n",
      "\n",
      "   worst symmetry  worst fractal dimension  y  \n",
      "0          0.4601                  0.11890  0  \n",
      "1          0.2750                  0.08902  0  \n",
      "2          0.3613                  0.08758  0  \n",
      "3          0.6638                  0.17300  0  \n",
      "4          0.2364                  0.07678  0  \n",
      "\n",
      "[5 rows x 31 columns]\n",
      "モデルデータのOriginalの特徴量(X): (569, 30)\n",
      "モデルデータの正解ラベルの個数\n",
      "モデルデータの欠損値の補完後の特徴量(X_ohe): (569, 30)\n",
      "モデルデータの特徴量選択後(X_ohe_selected): (569, 9)\n",
      "===============================================================================================================\n",
      "LogisticRegression :\t precision : 0.9686 +- 0.0042\n",
      "roc_auc: 0.9903 +- 0.0024 ,\t f1: 0.9604 +- 0.0073 ,\t accuracy: 0.9508 +- 0.009 ,\t precision: 0.9686 +- 0.0042 ,\t recall: 0.9524 +- 0.0105 ,\t f1_macro: 0.9477 +- 0.0095 \n",
      "\n",
      "KNeighborsClassifier :\t precision : 0.9581 +- 0.0115\n",
      "roc_auc: 0.9719 +- 0.0071 ,\t f1: 0.958 +- 0.0092 ,\t accuracy: 0.9473 +- 0.0113 ,\t precision: 0.9581 +- 0.0115 ,\t recall: 0.958 +- 0.0137 ,\t f1_macro: 0.9437 +- 0.0119 \n",
      "\n",
      "SVM :\t precision : 0.9658 +- 0.0135\n",
      "roc_auc: 0.9878 +- 0.0018 ,\t f1: 0.953 +- 0.0156 ,\t accuracy: 0.942 +- 0.0188 ,\t precision: 0.9658 +- 0.0135 ,\t recall: 0.9412 +- 0.0314 ,\t f1_macro: 0.9386 +- 0.0196 \n",
      "\n",
      "DecisionTreeClassifier :\t precision : 0.949 +- 0.0132\n",
      "roc_auc: 0.9254 +- 0.0183 ,\t f1: 0.942 +- 0.0162 ,\t accuracy: 0.928 +- 0.0193 ,\t precision: 0.949 +- 0.0132 ,\t recall: 0.9356 +- 0.026 ,\t f1_macro: 0.9234 +- 0.02 \n",
      "\n",
      "RandomForestClassifier :\t precision : 0.9627 +- 0.0044\n",
      "roc_auc: 0.983 +- 0.0058 ,\t f1: 0.9518 +- 0.0082 ,\t accuracy: 0.9403 +- 0.0099 ,\t precision: 0.9627 +- 0.0044 ,\t recall: 0.9412 +- 0.0119 ,\t f1_macro: 0.9366 +- 0.0102 \n",
      "\n",
      "RSVC :\t precision : 0.9658 +- 0.0135\n",
      "roc_auc: 0.9878 +- 0.0018 ,\t f1: 0.953 +- 0.0156 ,\t accuracy: 0.942 +- 0.0188 ,\t precision: 0.9658 +- 0.0135 ,\t recall: 0.9412 +- 0.0314 ,\t f1_macro: 0.9386 +- 0.0196 \n",
      "\n",
      "GradientBoostingClassifier :\t precision : 0.9685 +- 0.0044\n",
      "roc_auc: 0.9839 +- 0.0047 ,\t f1: 0.9587 +- 0.0167 ,\t accuracy: 0.9491 +- 0.0198 ,\t precision: 0.9685 +- 0.0044 ,\t recall: 0.9496 +- 0.0299 ,\t f1_macro: 0.946 +- 0.0204 \n",
      "\n",
      "GradientBoostingClassifier :\t precision : 0.9579 +- 0.0069\n",
      "roc_auc: 0.9856 +- 0.0061 ,\t f1: 0.9565 +- 0.008 ,\t accuracy: 0.9455 +- 0.0099 ,\t precision: 0.9579 +- 0.0069 ,\t recall: 0.9552 +- 0.0105 ,\t f1_macro: 0.9418 +- 0.0104 \n",
      "\n",
      "GradientBoostingClassifier :\t precision : 0.9685 +- 0.0044\n",
      "roc_auc: 0.9839 +- 0.0047 ,\t f1: 0.9587 +- 0.0167 ,\t accuracy: 0.9491 +- 0.0198 ,\t precision: 0.9685 +- 0.0044 ,\t recall: 0.9496 +- 0.0299 ,\t f1_macro: 0.946 +- 0.0204 \n",
      "\n",
      "XGBClassifier :\t precision : 0.9611 +- 0.0034\n",
      "roc_auc: 0.9825 +- 0.0036 ,\t f1: 0.9637 +- 0.0043 ,\t accuracy: 0.9543 +- 0.0049 ,\t precision: 0.9611 +- 0.0034 ,\t recall: 0.9664 +- 0.0119 ,\t f1_macro: 0.9511 +- 0.0049 \n",
      "\n",
      "XGBClassifier :\t precision : 0.9632 +- 0.0047\n",
      "roc_auc: 0.9874 +- 0.0019 ,\t f1: 0.9591 +- 0.0135 ,\t accuracy: 0.949 +- 0.0162 ,\t precision: 0.9632 +- 0.0047 ,\t recall: 0.9552 +- 0.0221 ,\t f1_macro: 0.9458 +- 0.0169 \n",
      "\n",
      "XGBClassifier :\t precision : 0.9524 +- 0.0029\n",
      "roc_auc: 0.9833 +- 0.0027 ,\t f1: 0.9523 +- 0.0093 ,\t accuracy: 0.9402 +- 0.0108 ,\t precision: 0.9524 +- 0.0029 ,\t recall: 0.9524 +- 0.021 ,\t f1_macro: 0.9362 +- 0.011 \n",
      "\n",
      "BaggingClassifier :\t precision : 0.9583 +- 0.0176\n",
      "roc_auc: 0.9839 +- 0.0053 ,\t f1: 0.958 +- 0.0119 ,\t accuracy: 0.9473 +- 0.0148 ,\t precision: 0.9583 +- 0.0176 ,\t recall: 0.958 +- 0.0182 ,\t f1_macro: 0.9436 +- 0.0158 \n",
      "\n",
      "MLPClassifier :\t precision : 0.9039 +- 0.0345\n",
      "roc_auc: 0.9822 +- 0.001 ,\t f1: 0.9414 +- 0.0151 ,\t accuracy: 0.9227 +- 0.0211 ,\t precision: 0.9039 +- 0.0345 ,\t recall: 0.9832 +- 0.0119 ,\t f1_macro: 0.9139 +- 0.0247 \n",
      "\n",
      "VotingClassifier :\t precision : 0.9533 +- 0.0097\n",
      "roc_auc: 0.984 +- 0.0065 ,\t f1: 0.9583 +- 0.0003 ,\t accuracy: 0.949 +- 0.0025 ,\t precision: 0.9481 +- 0.0095 ,\t recall: 0.9692 +- 0.004 ,\t f1_macro: 0.9415 +- 0.0022 \n",
      "\n",
      "===============================================================================================================\n",
      "Best algorithm: LogisticRegression\n",
      "Best precision score: 0.968635159459239\n",
      "Best_model: Pipeline(memory=None,\n",
      "     steps=[('scl', StandardScaler(copy=True, with_mean=True, with_std=True)), ('reduct', PCA(copy=True, iterated_power='auto', n_components=None, random_state=1,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', LogisticRegression(C=10, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=1,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))])\n",
      "===============================================================================================================\n",
      "LogisticRegression で grid_search を実行\n",
      "Best Score: 0.9769572993815181\n",
      "Best Params: {'est__C': 10.0, 'est__penalty': 'l1', 'pca__n_components': 5}\n",
      "Best Estimator: Pipeline(memory=None,\n",
      "     steps=[('scl', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, iterated_power='auto', n_components=5, random_state=1,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('est', LogisticRegression(C=10.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=1,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))])\n",
      "GridSearch 実行時間: 0.0 分\n",
      "confusion matrix:\n",
      " [[203   9]\n",
      " [ 18 339]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.96      0.94       212\n",
      "          1       0.97      0.95      0.96       357\n",
      "\n",
      "avg / total       0.95      0.95      0.95       569\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load and return the breast cancer wisconsin dataset (classification).\n",
    "# The breast cancer dataset is a classic and very easy binary classification dataset.\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "dataset = load_breast_cancer()\n",
    "\n",
    "# Set dataframe\n",
    "X = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
    "y = pd.DataFrame(dataset.target, columns=['y'])\n",
    "\n",
    "# check the shape\n",
    "print('----------------------------------------------------------------------------------------')\n",
    "print('X shape: (%i,%i)' %X.shape)\n",
    "print('y shape: (%i,%i)' %y.shape)\n",
    "print('----------------------------------------------------------------------------------------')\n",
    "print(y.groupby('y').size())\n",
    "print('y=0 means Marignant(悪性), y=1 means Benign(良性):')\n",
    "print('----------------------------------------------------------------------------------------')\n",
    "print(X.join(y).head())\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "# 2018/08/10 Randamizedsearch に変更\n",
    "# 2018/08/19 grid search にて、nested cv を実施\n",
    "### ご参考 https://qiita.com/msrks/items/e3e958c04a5167575c41\n",
    "\n",
    "# SET PARAMETERS\n",
    "### dm_for_model2\n",
    "#file_model = 'dm_for_model2.csv'\n",
    "#file_score = 'dm_for_fwd2.csv'\n",
    "#ohe_cols = ['mode_category']\n",
    "\n",
    "ohe_cols = []\n",
    "\n",
    "### av_loan\n",
    "#file_model = 'av_loan_u6lujuX_CVtuZ9i.csv'\n",
    "#file_score = 'av_loan_test_Y3wMUE5_7gLdaTN.csv'\n",
    "#ohe_cols = ['Gender',\n",
    "#\t\t\t'Married',\n",
    "#\t\t\t'Dependents',\n",
    "#\t\t\t'Education',\n",
    "#\t\t\t'Self_Employed',\n",
    "#\t\t\t'Credit_History',\n",
    "#\t\t\t'Property_Area'\n",
    "#\t\t\t]\n",
    "\n",
    "# Chose one of the followings scoreing parameters.\n",
    "# この解説がわかりやすい: https://blog.amedama.jp/entry/2017/12/18/005311\n",
    "#SCORE = 'accuracy' # Accuracy = (TP+TN)/(TP+FP+FN+TN)\n",
    "#SCORE = 'roc_auc'  # 偽陽性率と真陽性率で評価する指標\n",
    "SCORE = 'precision'# 正と予測したデータのうち，実際に正であるものの割合【正確性】適合率を重視するときは FN を許容できるケース.(FP があっては困るときに使う）\n",
    "#SCORE = 'recall'    # 実際に正であるもののうち，正であると予測されたものの割合 【網羅性】 再現率を重視するときは FP を許容できるケース. （FN があっては困るとき使う）\n",
    "#SCORE = 'f1'       # 適合率（PRE）と再現率（REC）の調和平均\n",
    "#SCORE = 'f1_macro'\n",
    "#SCORE = 'f1_micro'\n",
    "#SCORE = 'log_loss'\n",
    "# Valid options are ['accuracy', 'adjusted_mutual_info_score', 'adjusted_rand_score', 'average_precision', 'completeness_score', \n",
    "#'explained_variance', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'fowlkes_mallows_score', 'homogeneity_score', \n",
    "#'mutual_info_score', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', \n",
    "#'neg_median_absolute_error', 'normalized_mutual_info_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', \n",
    "#'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc', 'v_measure_score']\n",
    "# http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "\n",
    "# make dir for modeling file\n",
    "import os\n",
    "model_path = './model'\n",
    "if not os.path.exists(model_path ):\n",
    "    os.makedirs(model_path )\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning) \n",
    "\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler,Imputer,MinMaxScaler,RobustScaler,MaxAbsScaler\n",
    "from sklearn.feature_selection import RFECV, SelectFromModel\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier,VotingClassifier,AdaBoostClassifier,GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV,RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "# data load\n",
    "#df = pd.read_csv('./data/'+ file_model + '.csv', header=0)\n",
    "#df = pd.read_csv('./data/'+ file_model, header=0)\n",
    "#ID = df.iloc[:,0] \n",
    "#y = df.iloc[:,1]\n",
    "#X = df.iloc[:,2:]\n",
    "\n",
    "## TEST\n",
    "## print(y)\n",
    "## print(X)\n",
    "\n",
    "print('モデルデータのOriginalの特徴量(X):', X.shape)\n",
    "print('モデルデータの正解ラベルの個数')\n",
    "#print(y.value_counts())\n",
    "\n",
    "# preprocessing-1: one-hot encoding\n",
    "X_ohe = pd.get_dummies(X, dummy_na=True, columns=ohe_cols)\n",
    "X_ohe = X_ohe.dropna(axis=1, how='all')\n",
    "X_ohe_columns = X_ohe.columns.values\n",
    "\n",
    "# preprocessing-2: null imputation\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "imp.fit(X_ohe)\n",
    "X_ohe = pd.DataFrame(imp.transform(X_ohe), columns=X_ohe_columns)\n",
    "print('モデルデータの欠損値の補完後の特徴量(X_ohe):', X_ohe.shape)\n",
    "\n",
    "##########################################################################\n",
    "# ここでexhaustive search を実施して、最適な特徴量の選択を実施したが、\n",
    "# RFECVの結果と、exhaustive search の結果があまり変わらなかったので割愛する\n",
    "#from sklearn.feature_selection import RFE\n",
    "#pre_pipe = Pipeline([('pre_scl', StandardScaler()),\n",
    "#                     ('pre_mms', MinMaxScaler()),\n",
    "#                     ('pre_rs', RobustScaler()),\n",
    "#                     ('rfe', RFE(estimator=RandomForestClassifier(random_state=1),step=0.05))\n",
    "#                     ])\n",
    "#N_FEATURES_OPTIONS = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25]\n",
    "#pre_param_grid = [{'rfe__n_features_to_select': N_FEATURES_OPTIONS\n",
    "#            }]\n",
    "#\n",
    "#pre_gs = GridSearchCV(pre_pipe, cv=3, n_jobs=1, param_grid=pre_param_grid)\n",
    "#pre_gs = pre_gs.fit(X_ohe, y.as_matrix().ravel())\n",
    "#\n",
    "#print('Best Score:', pre_gs.best_score_)\n",
    "#print('Best Params', pre_gs.best_params_)\n",
    "##########################################################################\n",
    "\n",
    "# preprocessing-3: feature selection\n",
    "# 特徴量選択に関するご参考 https://hayataka2049.hatenablog.jp/entry/2018/01/25/075927\n",
    "#\t\t\t\t\t\t   https://www.kaggle.com/narumin/titanic-data-science-japanese-tutorial\n",
    "#\t\t\t\t\t\t   https://qiita.com/rockhopper/items/a68ceb3248f2b3a41c89\n",
    "### RFECVの場合\n",
    "#selector = RFECV(estimator=RandomForestClassifier(random_state=0), step=0.05, cv=3, scoring=SCORE)\n",
    "\n",
    "# scoring に f1や roc_aucを指定したところ、精度が変化\n",
    "##selector = RFECV(estimator=RandomForestClassifier(random_state=0), step=0.05) ### ORIGINAL \n",
    "##selector = RFECV(estimator=LogisticRegression(), step=0.05, cv=10, scoring='average_precision') NG\n",
    "##selector = RFECV(estimator=xgb.XGBClassifier(random_state=1), step=0.05, cv=10, scoring=SCORE)\n",
    "\n",
    "#selector.fit(X_ohe, y.as_matrix().ravel())\n",
    "#X_ohe_selected = selector.transform(X_ohe)\n",
    "#X_ohe_selected = pd.DataFrame(X_ohe_selected, columns=X_ohe_columns[selector.support_])\n",
    "#print('モデルデータの特徴量選択後(X_ohe_selected):',X_ohe_selected.shape)\n",
    "#X_ohe_selected.head()\n",
    "\n",
    "############################################################################################\n",
    "### SelectFromModelの場合\n",
    "##estimator = RandomForestClassifier(class_weight='balanced', random_state=0)\n",
    "estimator = RandomForestClassifier(random_state=0)\n",
    "estimator.fit(X_ohe, y.as_matrix().ravel())\n",
    "selector = SelectFromModel(estimator, threshold='mean', prefit=True)\n",
    "X_ohe_selected = selector.transform(X_ohe)\n",
    "X_ohe_selected = pd.DataFrame(X_ohe_selected, columns=X_ohe_columns[selector.get_support()])\n",
    "print('モデルデータの特徴量選択後(X_ohe_selected):',X_ohe_selected.shape)\n",
    "### X_ohe_selected.head()\n",
    "\n",
    "#print('全パラメータの重要度:', estimator.feature_importances_)\n",
    "## model profile NG\n",
    "#imp = pd.DataFrame(estimator.feature_importances_, columns=X_ohe_columns)\n",
    "#imp.T.to_csv('./data/feature_importances.csv', index=True)\n",
    "\n",
    "### RFE の場合\n",
    "#selector = RFE(estimator=RandomForestClassifier(random_state=1),n_features_to_select=10,step=0.05)\n",
    "#selector = RFE(estimator=RandomForestClassifier(random_state=1),n_features_to_select=pre_gs.best_params_,step=0.05)\n",
    "\n",
    "#selector.fit(X_ohe, y.as_matrix().ravel())\n",
    "\n",
    "#X_ohe_selected = selector.transform(X_ohe)\n",
    "#X_ohe_selected = pd.DataFrame(X_ohe_selected,columns=X_ohe_columns[selector.support_])\n",
    "\n",
    "#############################################################################################\n",
    "# preprocessing-4: preprocessing of a score data along with a model dataset\n",
    "#if len(file_score)>0:\n",
    "    # load score data\n",
    "#    dfs = pd.read_csv('./data/'+ file_score, header=0)\n",
    "#    IDs = dfs.iloc[:,[0]] \n",
    "#    Xs = dfs.iloc[:,1:]\n",
    "#Xs = X.iloc[201:,]\n",
    "#y = df.iloc[0:200,]\n",
    "#Xs_ohe = pd.get_dummies(Xs, dummy_na=True, columns=ohe_cols)\n",
    "#cols_m = pd.DataFrame(None, columns=X_ohe_columns, dtype=float)\n",
    "\n",
    "# consistent with columns set\n",
    "#Xs_exp = pd.concat([cols_m, Xs_ohe])\n",
    "#Xs_exp.loc[:,list(set(X_ohe_columns)-set(Xs_ohe.columns.values))] = \\\n",
    "#                        Xs_exp.loc[:,list(set(X_ohe_columns)-set(Xs_ohe.columns.values))].fillna(0, axis=1)\n",
    "#Xs_exp = Xs_exp.drop(list(set(Xs_ohe.columns.values)-set(X_ohe_columns)), axis=1)\n",
    "\n",
    "# re-order the score data columns\n",
    "#Xs_exp = Xs_exp.reindex_axis(X_ohe_columns, axis=1)\n",
    "#Xs_exp = pd.DataFrame(imp.transform(Xs_exp), columns=X_ohe_columns)\n",
    "\n",
    "#Xs_exp_selected = Xs_exp.loc[:,X_ohe_columns[selector.support_]] # RFECVの場合\n",
    "#Xs_exp_selected = Xs_exp.loc[:,X_ohe_columns[selector.get_support()]]\n",
    "\n",
    "# CLASSIFIER\n",
    "#model_name = 'GBC_001'\n",
    "#clf = Pipeline([('scl',StandardScaler()), ('est',GradientBoostingClassifier(random_state=1))])\n",
    "\n",
    "lr = Pipeline([('scl', StandardScaler()),\n",
    "#              ('mms', MinMaxScaler()),\n",
    "#              ('mas', MaxAbsScaler()),\n",
    "#              ('rs', RobustScaler()),\n",
    "               ('reduct',PCA(n_components=None, svd_solver='auto',random_state=1)),\n",
    "               ('clf', LogisticRegression(C=10, class_weight='balanced', random_state=1))])\n",
    "\n",
    "knn = Pipeline([('scl', StandardScaler()),\n",
    "#               ('mms', MinMaxScaler()),\n",
    "#               ('mas', MaxAbsScaler()),\n",
    "#               ('rs', RobustScaler()),\n",
    "                ('reduct',PCA(n_components=None, svd_solver='auto',random_state=1)),\n",
    "                ('clf', KNeighborsClassifier(algorithm='auto', \n",
    "                                                leaf_size=30, \n",
    "                                                metric='minkowski',\n",
    "                                                metric_params=None, \n",
    "                                                n_jobs=1, \n",
    "                                                n_neighbors=5, \n",
    "                                                p=2,\n",
    "                                                weights='uniform'))])\n",
    "\n",
    "svm = Pipeline([('scl', StandardScaler()),\n",
    "#               ('mms', MinMaxScaler()),\n",
    "#               ('mas', MaxAbsScaler()),\n",
    "#               ('rs', RobustScaler()),\n",
    "                ('reduct',PCA(n_components=None, svd_solver='auto',random_state=1)),\n",
    "                ('clf', SVC(kernel='rbf', C=1.0, class_weight='balanced', random_state=1))])\n",
    "\n",
    "dc = Pipeline([('scl', StandardScaler()),\n",
    "#              ('mms', MinMaxScaler()),\n",
    "#              ('mas', MaxAbsScaler()),\n",
    "#              ('rs', RobustScaler()),\n",
    "               ('reduct',PCA(n_components=None, svd_solver='auto',random_state=1)),\n",
    "               ('clf', DecisionTreeClassifier(class_weight='balanced', random_state=1))])\n",
    "\n",
    "rf = Pipeline([('scl', StandardScaler()),\n",
    "#              ('mms', MinMaxScaler()),\n",
    "#              ('mas', MaxAbsScaler()),\n",
    "#              ('rs', RobustScaler()),\n",
    "               ('reduct',PCA(n_components=None, svd_solver='auto',random_state=1)),\n",
    "               ('clf', RandomForestClassifier(class_weight='balanced', random_state=1))])\n",
    "\n",
    "rsvc = Pipeline([('scl',StandardScaler()),\n",
    "#                ('mms', MinMaxScaler()),\n",
    "#                ('mas', MaxAbsScaler()),\n",
    "#                ('rs', RobustScaler()),\n",
    "                 ('reduct',PCA(n_components=None, svd_solver='auto',random_state=1)),\n",
    "                 ('est',SVC(C=1.0, kernel='rbf', class_weight='balanced', random_state=1))])\n",
    "\n",
    "lsvc = Pipeline([('scl',StandardScaler()),\n",
    "#                ('mms', MinMaxScaler()),\n",
    "#                ('mas', MaxAbsScaler()),\n",
    "#                ('rs', RobustScaler()),\n",
    "                 ('reduct',PCA(n_components=None, svd_solver='auto',random_state=1)),\n",
    "                 ('est',LinearSVC(C=1.0, class_weight='balanced', random_state=1))])\n",
    "\n",
    "gb1 = Pipeline([('scl',StandardScaler()),\n",
    "#                ('mms', MinMaxScaler()),\n",
    "#                ('mas', MaxAbsScaler()),\n",
    "#                ('rs', RobustScaler()),\n",
    "                 ('reduct',PCA(n_components=None, svd_solver='auto',random_state=1)),\n",
    "                 ('est',GradientBoostingClassifier(random_state=1))])\n",
    "\n",
    "gb2 = Pipeline([('scl',StandardScaler()),\n",
    "#                ('mms', MinMaxScaler()),\n",
    "#                ('mas', MaxAbsScaler()),\n",
    "#                ('rs', RobustScaler()),\n",
    "                 ('reduct',PCA(n_components=None, svd_solver='auto',random_state=1)),\n",
    "                 ('est',GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,max_depth=1, random_state=1))])\n",
    "\n",
    "gb3 = Pipeline([('scl',StandardScaler()),\n",
    "#                ('mms', MinMaxScaler()),\n",
    "#                ('mas', MaxAbsScaler()),\n",
    "#                ('rs', RobustScaler()),\n",
    "                 ('reduct',PCA(n_components=None, svd_solver='auto',random_state=1)),\n",
    "                 ('est',GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,max_depth=3, random_state=1))])\n",
    "\n",
    "xgb1 = Pipeline([('scl',StandardScaler()),\n",
    "#                ('mms', MinMaxScaler()),\n",
    "#                ('mas', MaxAbsScaler()),\n",
    "#                ('rs', RobustScaler()),\n",
    "                 ('reduct',PCA(n_components=None, svd_solver='auto',random_state=1)),\n",
    "                 ('est', xgb.XGBClassifier(random_state=1))])\n",
    "\n",
    "xgb2 = Pipeline([('scl',StandardScaler()),\n",
    "#                ('mms', MinMaxScaler()),\n",
    "#                ('mas', MaxAbsScaler()),\n",
    "#                ('rs', RobustScaler()),\n",
    "                 ('reduct',PCA(n_components=None, svd_solver='auto',random_state=1)),\n",
    "                 ('est', xgb.XGBClassifier(n_estimators=100,learning_rate=0.1,colsample_bytree=0.8,\n",
    "                                           gamma=0,max_depth=5,min_child_weight=3, subsample=0.7,objective='binary:logistic',random_state=1))])\n",
    "\n",
    "xgb3 = Pipeline([('scl',StandardScaler()),\n",
    "#                ('mms', MinMaxScaler()),\n",
    "#                ('mas', MaxAbsScaler()),\n",
    "#                ('rs', RobustScaler()),\n",
    "                 ('reduct',PCA(n_components=None, svd_solver='auto',random_state=1)),\n",
    "                 ('est', xgb.XGBClassifier(n_estimators=1000,learning_rate=0.1,colsample_bytree=0.8,\n",
    "                                           gamma=0,max_depth=5,min_child_weight=3, subsample=0.7,objective='binary:logistic',random_state=1))])\n",
    "gnb = Pipeline([('scl',StandardScaler()),\n",
    "#                ('mms', MinMaxScaler()),\n",
    "#                ('mas', MaxAbsScaler()),\n",
    "#                ('rs', RobustScaler()),\n",
    "                 ('reduct',PCA(n_components=None, svd_solver='auto',random_state=1)),\n",
    "                 ('est',GaussianNB())])\n",
    "\n",
    "bag = Pipeline([('scl',StandardScaler()),\n",
    "#                ('mms', MinMaxScaler()),\n",
    "#                ('mas', MaxAbsScaler()),\n",
    "#                ('rs', RobustScaler()),\n",
    "                 ('reduct',PCA(n_components=None, svd_solver='auto',random_state=1)),\n",
    "                 ('est',BaggingClassifier(DecisionTreeClassifier(class_weight='balanced', random_state=1),n_estimators = 100, max_features = 0.5,random_state=1))])\n",
    "\n",
    "mlp = Pipeline([('scl',StandardScaler()),\n",
    "#                ('mms', MinMaxScaler()),\n",
    "#                ('mas', MaxAbsScaler()),\n",
    "#                ('rs', RobustScaler()),\n",
    "                 ('reduct',PCA(n_components=None, svd_solver='auto',random_state=1)),\n",
    "                 ('est',MLPClassifier(hidden_layer_sizes=(10,5),max_iter=100,random_state=1))])\n",
    "\n",
    "clf1 = AdaBoostClassifier(n_estimators=10)\n",
    "clf2 = ExtraTreesClassifier(n_estimators=10, n_jobs=-1, criterion='gini',max_depth=1)\n",
    "clf3 = xgb.XGBClassifier(n_estimators=10, nthread=-1, max_depth = 1, seed=1)\n",
    "clf4 = GradientBoostingClassifier(n_estimators=10)\n",
    "clf5 = RandomForestClassifier(max_depth = 1,class_weight='balanced', random_state=1)\n",
    "\n",
    "vot = Pipeline([('scl',StandardScaler()),\n",
    "#                ('mms', MinMaxScaler()),\n",
    "#                ('mas', MaxAbsScaler()),\n",
    "#                ('rs', RobustScaler()),\n",
    "                 ('reduct',PCA(n_components=None, svd_solver='auto',random_state=1)),\n",
    "                 ('est',VotingClassifier(estimators=[('ab', clf1), ('etc', clf2), ('xgb', clf3),('gbc', clf4)], weights=[1,1,1,1], voting='soft'))])\n",
    "\n",
    "vot2 = Pipeline([('scl',StandardScaler()),\n",
    "#                ('mms', MinMaxScaler()),\n",
    "#                ('mas', MaxAbsScaler()),\n",
    "#                ('rs', RobustScaler()),\n",
    "                 ('reduct',PCA(n_components=None, svd_solver='auto',random_state=1)),\n",
    "                 ('est',VotingClassifier(estimators=[('ab', clf1), ('etc', clf2), ('xgb', clf3),('gbc', clf4),('gnb', clf5)], weights=[1,1,1,1,1], voting='soft'))])\n",
    "\n",
    "#models = [lr, knn, svm, dc, rf, rsvc, lsvc, gb1, gb2, gb3, xgb1, xgb2, xgb3, gnb, bgc, mlp]\n",
    "models = [lr, knn, svm, dc, rf, rsvc, gb1, gb2, gb3, xgb1, xgb2, xgb3, bag, mlp, vot]\n",
    "model_names = ['LogisticRegression',\n",
    "               'KNeighborsClassifier',\n",
    "               'SVM',\n",
    "               'DecisionTreeClassifier',\n",
    "               'RandomForestClassifier',\n",
    "               'RSVC',\n",
    "#               'LinearSVC',\n",
    "               'GradientBoostingClassifier',\n",
    "               'GradientBoostingClassifier',\n",
    "               'GradientBoostingClassifier',\n",
    "               'XGBClassifier',\n",
    "               'XGBClassifier',\n",
    "               'XGBClassifier',\n",
    "#               'GaussianNB',\n",
    "               'BaggingClassifier',\n",
    "               'MLPClassifier',\n",
    "               'VotingClassifier',\n",
    "#               'VotingClassifier'\n",
    "              ]\n",
    "\n",
    "#for loop in range(100):\n",
    "#    x_train, x_test, y_train, y_test = train_test_split(X_ohe_selected,\n",
    "#                                                        y.as_matrix().ravel(),\n",
    "#                                                        test_size=0.2)\n",
    "#with open('./data/' + 'model_score_list.csv', 'w') as f:\n",
    "#writer = csv.writer(f, lineterminator='\\n') # 改行コード（\\n）を指定しておく\n",
    "#writer.writerows([[round(np.average(results), 4), model_name]]) # 2次元配列も書き込める\n",
    "\n",
    "print('===============================================================================================================')\n",
    "\n",
    "BEST_SCORE = 0\n",
    "MODEL_RANKING = pd.DataFrame()\n",
    "### https://hayataka2049.hatenablog.jp/entry/2018/03/14/112454 #評価指標について\n",
    "for model_name, model in zip(model_names, models):\n",
    "    model.fit(X_ohe_selected, y.values.ravel())\n",
    "    train_score = model.score(X_ohe_selected, y.values.ravel())\n",
    "    test_score = model.score(X_ohe_selected, y.values.ravel())\n",
    " \n",
    "    results = cross_val_score(model, X_ohe_selected, y.values.ravel(), scoring=SCORE, cv=3)\n",
    "    results1 = cross_val_score(model, X_ohe_selected, y.values.ravel(), scoring='roc_auc', cv=3)\n",
    "    results2 = cross_val_score(model, X_ohe_selected, y.values.ravel(), scoring='f1_macro', cv=3) \n",
    "    results3 = cross_val_score(model, X_ohe_selected, y.values.ravel(), scoring='accuracy', cv=3) \n",
    "    results4 = cross_val_score(model, X_ohe_selected, y.values.ravel(), scoring='f1', cv=3) \n",
    "    results5 = cross_val_score(model, X_ohe_selected, y.values.ravel(), scoring='precision', cv=3) \n",
    "    results6 = cross_val_score(model, X_ohe_selected, y.values.ravel(), scoring='recall', cv=3) \n",
    "    \n",
    "    #results = cross_val_score(model, X_ohe_selected, y.values.ravel(), scoring=score)\n",
    "#    print(SCORE, 'score:\\t', round(np.average(results), 4), '+-', round(np.std(results), 4),',\\t', model_name)\n",
    "#    print('===============================================================================================================')\n",
    "\n",
    "    print(model_name, ':\\t', SCORE, ':', round(np.average(results), 4), '+-', round(np.std(results), 4))    \n",
    "    print('roc_auc:', round(np.average(results1), 4), '+-', round(np.std(results1), 4),',\\t',\n",
    "          'f1:', round(np.average(results4), 4), '+-', round(np.std(results4), 4),',\\t',\n",
    "          'accuracy:', round(np.average(results3), 4), '+-', round(np.std(results3), 4),',\\t',\n",
    "          'precision:', round(np.average(results5), 4), '+-', round(np.std(results5), 4),',\\t',\n",
    "          'recall:', round(np.average(results6), 4), '+-', round(np.std(results6), 4),',\\t',\n",
    "          'f1_macro:', round(np.average(results2), 4), '+-', round(np.std(results2), 4)\n",
    "          ,'\\n')\n",
    "#    print(classification_report(y.as_matrix().ravel(), model.predict(X_ohe_selected)))\n",
    "\n",
    "    MODEL_SCORE_LIST = pd.DataFrame([[model_name, SCORE, round(np.average(results), 4), model]] )\n",
    "    MODEL_SCORE_LIST.to_csv('./data/' + 'model_score_list.csv', mode='a', header=False, index=False)\n",
    "    \n",
    "    if BEST_SCORE < np.average(results):\n",
    "        BEST_SCORE = np.average(results)\n",
    "        BEST_ALGORITHM = model_name\n",
    "        BEST_MODEL = model\n",
    "print('===============================================================================================================')\n",
    "print('Best algorithm:', BEST_ALGORITHM)\n",
    "print('Best', SCORE ,'score:', BEST_SCORE)\n",
    "print('Best_model:', BEST_MODEL)\n",
    "print('===============================================================================================================')\n",
    "print(BEST_ALGORITHM, 'で grid_search を実行')\n",
    "\n",
    "# TEST \n",
    "#BEST_ALGORITHM = 'RandomForestClassifier'\n",
    "# xgboost のvoting ensenble\n",
    "# https://www.kaggle.com/pablonieto/eeg-analysis-voting-ensemble-python-2-7\n",
    "# round(len(X_ohe_selected.columns)*0.2)\n",
    "if BEST_ALGORITHM == 'LogisticRegression' :\n",
    "#4 x 11 x 2 = 88通り\n",
    "    GRID_EST = LogisticRegression(class_weight='balanced', random_state=1)\n",
    "    GRID_PARAM = {'pca__n_components':[round(len(X_ohe_selected.columns)*0.1),\n",
    "                                       round(len(X_ohe_selected.columns)*0.2),\n",
    "                                       round(len(X_ohe_selected.columns)*0.3),\n",
    "                                       round(len(X_ohe_selected.columns)*0.4),\n",
    "                                       round(len(X_ohe_selected.columns)*0.5),\n",
    "                                       round(len(X_ohe_selected.columns)*0.6),\n",
    "                                       round(len(X_ohe_selected.columns)*0.7),\n",
    "                                       round(len(X_ohe_selected.columns)*0.8),\n",
    "                                       round(len(X_ohe_selected.columns)*0.9),\n",
    "                                       None],\n",
    "                  'est__C':[1e-4, 1e-3, 1e-2, 1e-1, 0.5, 1., 5., 10., 15., 20., 25.],\n",
    "                  'est__penalty':['l1','l2']\n",
    "                 }\n",
    "# http://kamonohashiperry.com/archives/209\n",
    "# https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "# https://github.com/aarshayj/Analytics_Vidhya/blob/master/Articles/Parameter_Tuning_XGBoost_with_Example/XGBoost%20models.ipynb\n",
    "elif BEST_ALGORITHM == 'XGBClassifier' :\n",
    "#4 x 3 x 11 x 11 x 3 x 3 x 10 x 10 x 2 = 2613600通り\n",
    "    GRID_EST = xgb.XGBClassifier(objective='binary:logistic', random_state=1)\n",
    "    GRID_PARAM = {'pca__n_components':[round(len(X_ohe_selected.columns)*0.1),\n",
    "                                       round(len(X_ohe_selected.columns)*0.2),\n",
    "                                       round(len(X_ohe_selected.columns)*0.3),\n",
    "                                       round(len(X_ohe_selected.columns)*0.4),\n",
    "                                       round(len(X_ohe_selected.columns)*0.5),\n",
    "                                       round(len(X_ohe_selected.columns)*0.6),\n",
    "                                       round(len(X_ohe_selected.columns)*0.7),\n",
    "                                       round(len(X_ohe_selected.columns)*0.8),\n",
    "                                       round(len(X_ohe_selected.columns)*0.9),\n",
    "                                       None],\n",
    "                  'est__learning_rate': [0.01,0.1,0.3],\n",
    "                  'est__max_depth': sp_randint(1, 11),\n",
    "                  'est__min_child_weight': sp_randint(1, 11),\n",
    "                  'est__gamma':[0,0.5,1],\n",
    "                  'est__subsample': [0.5, 0.7, 0.9], \n",
    "                  'est__colsample_bytree': [0.5, 0.7, 0.9],\n",
    "                  'est__reg_alpha': sp_randint(1, 10),\n",
    "                  'est__reg_lambda': sp_randint(1, 10),\n",
    "                  'est__scale_pos_weight':sp_randint(1, 10), # useful for unbalanced\n",
    "                  'est__max_delta_step':sp_randint(1, 10), # useful for unbalanced\n",
    "                  'est__n_estimators': [1000]\n",
    "                 }\n",
    "\n",
    "elif BEST_ALGORITHM == 'GradientBoostingClassifier' :\n",
    "# 4 x 4 x 11 x 19 x 20 x 3 x 4 = 802560通り\n",
    "    GRID_EST = GradientBoostingClassifier(random_state=1)\n",
    "    GRID_PARAM = {'pca__n_components':[round(len(X_ohe_selected.columns)*0.1),\n",
    "                                       round(len(X_ohe_selected.columns)*0.2),\n",
    "                                       round(len(X_ohe_selected.columns)*0.3),\n",
    "                                       round(len(X_ohe_selected.columns)*0.4),\n",
    "                                       round(len(X_ohe_selected.columns)*0.5),\n",
    "                                       round(len(X_ohe_selected.columns)*0.6),\n",
    "                                       round(len(X_ohe_selected.columns)*0.7),\n",
    "                                       round(len(X_ohe_selected.columns)*0.8),\n",
    "                                       round(len(X_ohe_selected.columns)*0.9),\n",
    "                                       None],\n",
    "                  'est__learning_rate': [1e-3, 1e-2, 1e-1, 0.5],\n",
    "                  'est__max_depth': sp_randint(1, 11),\n",
    "                  'est__min_samples_split': sp_randint(2, 21),\n",
    "                  'est__min_samples_leaf': sp_randint(1, 21),\n",
    "                  'est__subsample': [0.5, 0.7, 0.9],\n",
    "                  'est__max_features': [0.1, 0.3, 0.5, 1],\n",
    "                  'est__n_estimators': [100]\n",
    "                 }\n",
    "    \n",
    "elif BEST_ALGORITHM == 'RandomForestClassifier' :\n",
    "# 4 x 3 x 3 x 15 x 2 = 1080\n",
    "    GRID_EST = RandomForestClassifier(class_weight='balanced',random_state=1)\n",
    "    GRID_PARAM = {'pca__n_components':[round(len(X_ohe_selected.columns)*0.1),\n",
    "                                       round(len(X_ohe_selected.columns)*0.2),\n",
    "                                       round(len(X_ohe_selected.columns)*0.3),\n",
    "                                       round(len(X_ohe_selected.columns)*0.4),\n",
    "                                       round(len(X_ohe_selected.columns)*0.5),\n",
    "                                       round(len(X_ohe_selected.columns)*0.6),\n",
    "                                       round(len(X_ohe_selected.columns)*0.7),\n",
    "                                       round(len(X_ohe_selected.columns)*0.8),\n",
    "                                       round(len(X_ohe_selected.columns)*0.9),\n",
    "                                       None],\n",
    "                  'est__n_estimators': [100, 200, 500],\n",
    "                  'est__max_features': ['auto', 'sqrt', 'log2'],\n",
    "                  'est__max_depth' : [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15],\n",
    "                  'est__criterion' :['gini', 'entropy']\n",
    "                 }\n",
    "elif BEST_ALGORITHM == 'SVM' :\n",
    "# 3 x 7 x 9 = 189通り\n",
    "    GRID_EST = SVC(kernel='rbf', C=1.0,probability=True, class_weight='balanced', random_state=1)\n",
    "    GRID_PARAM = {'pca__n_components':[round(len(X_ohe_selected.columns)*0.1),\n",
    "                                       round(len(X_ohe_selected.columns)*0.2),\n",
    "                                       round(len(X_ohe_selected.columns)*0.3),\n",
    "                                       round(len(X_ohe_selected.columns)*0.4),\n",
    "                                       round(len(X_ohe_selected.columns)*0.5),\n",
    "                                       round(len(X_ohe_selected.columns)*0.6),\n",
    "                                       round(len(X_ohe_selected.columns)*0.7),\n",
    "                                       round(len(X_ohe_selected.columns)*0.8),\n",
    "                                       round(len(X_ohe_selected.columns)*0.9),\n",
    "                                       None],\n",
    "#                  'est__kernel': ['rbf','sigmoid','linear'], \n",
    "                  'est__kernel': ['rbf'], \n",
    "#                  'est__class_weight':['None','balanced'],\n",
    "\t\t\t\t  'est__C': [0.001, 0.01, 0.1, 1,10,100,1000],\n",
    "                  'est__gamma': [1e-5,1e-4,0.001, 0.01, 0.1, 1,10,100,1000]\n",
    "                 }\n",
    "elif BEST_ALGORITHM == 'SVC' : \n",
    "# 4 x 3 x 7 x 9 = 189通り\n",
    "    GRID_EST = SVC(probability=True, random_state=1)\n",
    "    GRID_PARAM = {'pca__n_components':[round(len(X_ohe_selected.columns)*0.1),\n",
    "                                       round(len(X_ohe_selected.columns)*0.2),\n",
    "                                       round(len(X_ohe_selected.columns)*0.3),\n",
    "                                       round(len(X_ohe_selected.columns)*0.4),\n",
    "                                       round(len(X_ohe_selected.columns)*0.5),\n",
    "                                       round(len(X_ohe_selected.columns)*0.6),\n",
    "                                       round(len(X_ohe_selected.columns)*0.7),\n",
    "                                       round(len(X_ohe_selected.columns)*0.8),\n",
    "                                       round(len(X_ohe_selected.columns)*0.9),\n",
    "                                       None],\n",
    "#                  'est__kernel': ['rbf','sigmoid','linear'], \n",
    "                  'est__kernel': ['rbf'], \n",
    "#                  'est__class_weight':['None','balanced'],\n",
    "                  'est__gamma': [1e-5,1e-4,0.001, 0.01, 0.1, 1,10,100,1000],\n",
    "                  'est__C': [0.001, 0.01, 0.1, 1,10,100,1000]\n",
    "                 }\n",
    "elif BEST_ALGORITHM == 'RSVC' :\n",
    "    GRID_EST = SVC(C=1.0, kernel='rbf', random_state=1,probability=True)\n",
    "    GRID_PARAM = {'pca__n_components':[round(len(X_ohe_selected.columns)*0.1),\n",
    "                                       round(len(X_ohe_selected.columns)*0.2),\n",
    "                                       round(len(X_ohe_selected.columns)*0.3),\n",
    "                                       round(len(X_ohe_selected.columns)*0.4),\n",
    "                                       round(len(X_ohe_selected.columns)*0.5),\n",
    "                                       round(len(X_ohe_selected.columns)*0.6),\n",
    "                                       round(len(X_ohe_selected.columns)*0.7),\n",
    "                                       round(len(X_ohe_selected.columns)*0.8),\n",
    "                                       round(len(X_ohe_selected.columns)*0.9),\n",
    "                                       None],\n",
    "                  'est__kernel': ['rbf'], \n",
    "#                  'est__class_weight':['None','balanced'],\n",
    "                  'est__C': [0.001, 0.01, 0.1, 1,10,100,1000],\n",
    "                  'est__gamma': [1e-5,1e-4,0.001, 0.01, 0.1, 1,10,100,1000]\n",
    "                 }\n",
    "\n",
    "elif (BEST_ALGORITHM == 'MLPClassifier'):\n",
    "    GRID_EST = MLPClassifier(random_state=1)\n",
    "    GRID_PARAM = {'pca__n_components':[round(len(X_ohe_selected.columns)*0.1),\n",
    "                                       round(len(X_ohe_selected.columns)*0.2),\n",
    "                                       round(len(X_ohe_selected.columns)*0.3),\n",
    "                                       round(len(X_ohe_selected.columns)*0.4),\n",
    "                                       round(len(X_ohe_selected.columns)*0.5),\n",
    "                                       round(len(X_ohe_selected.columns)*0.6),\n",
    "                                       round(len(X_ohe_selected.columns)*0.7),\n",
    "                                       round(len(X_ohe_selected.columns)*0.8),\n",
    "                                       round(len(X_ohe_selected.columns)*0.9),\n",
    "                                       None],\n",
    "                  'est__hidden_layer_sizes':[(100,10),(200,10),(500,10),(100,20),(200,20),(500,20)],\n",
    "                  'est__max_iter':[10, 100, 1000],\n",
    "                  'est__batch_size':[10,50,100],\n",
    "                  'est__early_stopping':[True]\n",
    "                 }\n",
    "elif (BEST_ALGORITHM == 'KNeighborsClassifier'):\n",
    "    GRID_EST = KNeighborsClassifier(class_weight='balanced', random_state=1)\n",
    "    GRID_PARAM = {'pca__n_components':[round(len(X_ohe_selected.columns)*0.1),\n",
    "                                       round(len(X_ohe_selected.columns)*0.2),\n",
    "                                       round(len(X_ohe_selected.columns)*0.3),\n",
    "                                       round(len(X_ohe_selected.columns)*0.4),\n",
    "                                       round(len(X_ohe_selected.columns)*0.5),\n",
    "                                       round(len(X_ohe_selected.columns)*0.6),\n",
    "                                       round(len(X_ohe_selected.columns)*0.7),\n",
    "                                       round(len(X_ohe_selected.columns)*0.8),\n",
    "                                       round(len(X_ohe_selected.columns)*0.9),\n",
    "                                       None],\n",
    "                  'est__n_neighbors': [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "                 }\n",
    "elif (BEST_ALGORITHM == 'DecisionTreeClassifier'):\n",
    "    GRID_EST = DecisionTreeClassifier(class_weight='balanced', random_state=1)\n",
    "    GRID_PARAM = {'pca__n_components':[round(len(X_ohe_selected.columns)*0.1),\n",
    "                                       round(len(X_ohe_selected.columns)*0.2),\n",
    "                                       round(len(X_ohe_selected.columns)*0.3),\n",
    "                                       round(len(X_ohe_selected.columns)*0.4),\n",
    "                                       round(len(X_ohe_selected.columns)*0.5),\n",
    "                                       round(len(X_ohe_selected.columns)*0.6),\n",
    "                                       round(len(X_ohe_selected.columns)*0.7),\n",
    "                                       round(len(X_ohe_selected.columns)*0.8),\n",
    "                                       round(len(X_ohe_selected.columns)*0.9),\n",
    "                                       None],\n",
    "                  'est__max_depth': [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15],\n",
    "                  'est__criterion': ['gini', 'entropy']\n",
    "                 }\n",
    "elif (BEST_ALGORITHM == 'BaggingClassifier'):\n",
    "    GRID_EST = BaggingClassifier(DecisionTreeClassifier(),n_estimators = 100, max_features = 0.5)\n",
    "    GRID_PARAM = {'pca__n_components':[round(len(X_ohe_selected.columns)*0.1),\n",
    "                                       round(len(X_ohe_selected.columns)*0.2),\n",
    "                                       round(len(X_ohe_selected.columns)*0.3),\n",
    "                                       round(len(X_ohe_selected.columns)*0.4),\n",
    "                                       round(len(X_ohe_selected.columns)*0.5),\n",
    "                                       round(len(X_ohe_selected.columns)*0.6),\n",
    "                                       round(len(X_ohe_selected.columns)*0.7),\n",
    "                                       round(len(X_ohe_selected.columns)*0.8),\n",
    "                                       round(len(X_ohe_selected.columns)*0.9),\n",
    "                                       None],\n",
    "                  'est__base_estimator__max_depth' : [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15],\n",
    "                  'est__max_samples' : [0.05, 0.1, 0.2, 0.3, 0.5]\n",
    "                 }\n",
    "elif (BEST_ALGORITHM == 'VotingClassifier'):\n",
    "    GRID_EST = VotingClassifier(estimators=[('ab', clf1), ('etc', clf2), ('xgb', clf3),('gbc', clf4)], weights=[1,1,1,1], voting='soft')\n",
    "    # Voting のgridsearchの例\n",
    "    # https://qiita.com/yagays/items/a503117bd06bb938fdb9\n",
    "    GRID_PARAM = {'pca__n_components':[round(len(X_ohe_selected.columns)*0.1),\n",
    "                                       round(len(X_ohe_selected.columns)*0.2),\n",
    "                                       round(len(X_ohe_selected.columns)*0.3),\n",
    "                                       round(len(X_ohe_selected.columns)*0.4),\n",
    "                                       round(len(X_ohe_selected.columns)*0.5),\n",
    "                                       round(len(X_ohe_selected.columns)*0.6),\n",
    "                                       round(len(X_ohe_selected.columns)*0.7),\n",
    "                                       round(len(X_ohe_selected.columns)*0.8),\n",
    "                                       round(len(X_ohe_selected.columns)*0.9),\n",
    "                                       None]\n",
    "                 }\n",
    "\n",
    "GRID_PIPE = Pipeline([('scl', StandardScaler()),\n",
    "#                     ('mms', MinMaxScaler()),\n",
    "#                     ('mas', MaxAbsScaler()),\n",
    "#                     ('rs', RobustScaler()),\n",
    "                      ('pca', PCA(random_state=1)),\n",
    "                      ('est', GRID_EST)])\n",
    "\n",
    "# https://qiita.com/ragAgar/items/2f6bebdba5f9d7381310 # ランダムサーチの例について\n",
    "\n",
    "if BEST_ALGORITHM == 'XGBClassifier' or BEST_ALGORITHM == 'GradientBoostingClassifier' or BEST_ALGORITHM == 'RandomForestClassifier' :\n",
    "\tgs= RandomizedSearchCV( estimator=GRID_PIPE,\n",
    "                                  param_distributions=GRID_PARAM,\n",
    "                                  cv=3,                 #CV\n",
    "                                  n_iter=1000,           #interation num\n",
    "                                  scoring=SCORE,        #metrics\n",
    "                                  random_state=1)\n",
    "                                  \n",
    "else:\n",
    "\t# GridSearchCVのパイプラインの設定\n",
    "\tgs = GridSearchCV(estimator=GRID_PIPE,\n",
    "\t                  param_grid=GRID_PARAM,\n",
    "\t                  scoring=SCORE,\n",
    "\t                  cv=3)\n",
    "\n",
    "start = time.time()\n",
    "gs = gs.fit(X_ohe_selected, y.as_matrix().ravel())\n",
    "\n",
    "# 探索した結果のベストスコアとパラメータの取得\n",
    "print('Best Score:',gs.best_score_)\n",
    "print('Best Params:', gs.best_params_)\n",
    "print('Best Estimator:',gs.best_estimator_)\n",
    "\n",
    "end   = time.time()\n",
    "#print(start)\n",
    "#print(end)\n",
    "print('GridSearch 実行時間:',round((end - start)/60, 1), '分')\n",
    "\n",
    "#print('===============================================================================================================')\n",
    "#print('nested cross-validation のスコア')\n",
    "#start2 = time.time()\n",
    "#nested_cv_scores = cross_val_score(gs,\n",
    "#                            X_ohe_selected,\n",
    "#                            y.as_matrix().ravel(),\n",
    "#                            scoring=SCORE,\n",
    "#                            cv=3\n",
    "#                            )\n",
    "#\n",
    "#print(BEST_ALGORITHM, ':\\t', SCORE, ':', round(np.average(nested_cv_scores), 4), '+-', round(np.std(nested_cv_scores), 4))\n",
    "#end2   = time.time()\n",
    "#print('Nested cross-validation 実行時間:',round((end2 - start2)/60, 1), '分')\n",
    "\n",
    "#CM = confusion_matrix(y_true, y_pred)\n",
    "CM = confusion_matrix(y.as_matrix().ravel(), gs.predict(X_ohe_selected))\n",
    "print('confusion matrix:\\n', CM)\n",
    "print(classification_report(y.as_matrix().ravel(), gs.predict(X_ohe_selected)))\n",
    "\n",
    "GS_SCORE_LIST = pd.DataFrame([[GRID_EST, SCORE, gs.best_score_, gs.best_params_]] )\n",
    "GS_SCORE_LIST.to_csv('./data/' + 'grid_score_list.csv', mode='a', header=False, index=False)\n",
    "joblib.dump(gs, './model/'+ BEST_ALGORITHM + '.pkl')\n",
    "\n",
    "filename=BEST_ALGORITHM + '.pkl'\n",
    "\n",
    "#保存したモデルをロード\n",
    "loaded_model = joblib.load(open('./model/'+ filename, 'rb'))\n",
    "\n",
    "#score = pd.DataFrame(gs.predict_proba(Xs_exp_selected)[:,1], columns=['pred_score'])\n",
    "#IDs.join(score).to_csv('./data/'+  model_name + '_' + file_score + '_with_pred.csv', index=False)\n",
    "\n",
    "#cv_result.csvを格納\n",
    "#pd.DataFrame(gs.cv_results_).to_csv('./data/'+  model_name + '_' + file_score + 'cv_result.csv', index=False)\n",
    "#AttributeError: 'GridSearchCV' object has no attribute 'cv_results_'\n",
    "\n",
    "##############################################################################################\n",
    "# modeling\n",
    "#clf.fit(X_ohe_selected, y.as_matrix().ravel())\n",
    "#joblib.dump(clf, './model/'+ model_name + '.pkl')\n",
    "#results = cross_val_score(clf, X_ohe_selected, y.as_matrix().ravel(), scoring=score, cv=5)\n",
    "#print('cv score:', np.average(results), '+-', np.std(results))\n",
    "\n",
    "# scoring\n",
    "#if len(file_score)>0:\n",
    "#    score = pd.DataFrame(clf.predict_proba(Xs_exp_selected)[:,1], columns=['pred_score'])\n",
    "#    IDs.join(score).to_csv('./data/'+  model_name + '_' + file_score + '_with_pred.csv', index=False)\n",
    "\n",
    "# model profile\n",
    "#imp = pd.DataFrame([clf.named_steps['est'].feature_importances_], columns=X_ohe_columns[selector.support_])\n",
    "#imp.T.to_csv('./data/'+  model_name + '_feature_importances.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
